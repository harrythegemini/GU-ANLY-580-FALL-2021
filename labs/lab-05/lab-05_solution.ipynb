{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514fffaf-36ee-4deb-802b-4685c2dcb1c6",
   "metadata": {},
   "source": [
    "# Lab-05: Visualizing tweets from the 2020 US presidential election"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82dc807-629b-47e6-997d-c77bfb105cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset is a randomly sampled subset of: https://www.kaggle.com/manchunhui/us-election-2020-tweets\n",
    "trump = pd.read_csv(\"2020_tweets_trump.csv\", lineterminator='\\n')\n",
    "biden = pd.read_csv(\"2020_tweets_biden.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbac270-263e-4481-b7b6-a08aee76a7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biden), len(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f5d88e-ae90-41cf-be32-00579cf35fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10000\n",
    "trump = trump.sample(n=M//2)\n",
    "biden = biden.sample(n=M//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50310e26-a410-42a1-81fb-be0b5ac32905",
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_tweets = biden['tweet'].tolist()\n",
    "trump_tweets = trump['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef537b0e-ca78-44af-bce3-0a9a21ba79d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#CNN u ekstazi. #Biden'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biden_tweets[3023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "396502e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(biden_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3b36a-9598-4efb-b3ad-b0cccd331bb2",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d4a8f95-2929-404c-bc7e-695b617f883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "pipeline_name = '2020ElectionTweets'\n",
    "\n",
    "\n",
    "def camel_case_split(str):\n",
    "    \"\"\" This function turns in #Biden2020 into Biden 2020 \"\"\"\n",
    "    return \" \".join([wrd for wrd in re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', str)])\n",
    "\n",
    "\n",
    "@Language.component(pipeline_name)\n",
    "def preprocess(doc):\n",
    "    doc = [token for token in doc if not token.is_punct]\n",
    "    # doc = [token for token in doc if not token.is_stop]\n",
    "    doc = [token.text.lower().strip() for token in doc]\n",
    "    doc = [token for token in doc if 0 < len(token) <= 12]\n",
    "    return \" \".join(doc)\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \n",
    "    # http://emailregex.com/\n",
    "    email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)\n",
    "    *|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]\n",
    "    |\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9]\n",
    "    (?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}\n",
    "    (?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:\n",
    "    (?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "    # replace = [ (pattern-to-replace, replacement),  ...]\n",
    "    replace = [\n",
    "        (\"<[^>]*>\", \" \"),\n",
    "        (email_re, \" \"),                           # Matches emails\n",
    "        (r\"(?<=\\d),(?=\\d)\", \"\"),                   # Remove commas in numbers\n",
    "        (r\"\\d+\", \" \"),                             # Map digits to special token <numbr>\n",
    "        (r\"[*\\^\\.$&@<>,\\-/+{|}=?#:;'\\\"\\[\\]]\", \"\"), # Punctuation and other junk\n",
    "        (r\"[\\n\\t\\r]\", \" \"),                        # Removes newlines, tabs, creturn\n",
    "        (r\"[^\\x00-\\x7F]+\", \"\"),                    # Removes non-ascii chars\n",
    "        (r\"\\\\+\", \" \"),                             # Removes double-backslashs\n",
    "        (r\"\\s+n\\s+\", \" \"),                         # 'n' leftover from \\\\n\n",
    "        (r\"\\s+\", \" \")                              # Strips extra whitespace\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline = spacy.load('en_core_web_sm')\n",
    "        self.pipeline.add_pipe(pipeline_name);\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.transform(*args, **kwargs)\n",
    "\n",
    "    def transform(self, doc: str):\n",
    "        for repl in self.replace:\n",
    "            doc = re.sub(repl[0], repl[1], doc)\n",
    "        doc = camel_case_split(doc)\n",
    "        return self.pipeline(doc)\n",
    "    \n",
    "pipeline = Pipeline();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e250385f-d94b-4110-ac61-6e4a638aa02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:41<00:00, 120.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with tqdm(total=M//2) as bar:\n",
    "    for i, (bt, tt) in enumerate(zip(biden_tweets, trump_tweets)):\n",
    "        biden_tweets[i] = pipeline(bt)\n",
    "        trump_tweets[i] = pipeline(tt)\n",
    "        bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dafb08c-1348-4fca-bf4e-d304d67fcc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us election results joe biden us elections on ab us elections ym vo ev'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biden_tweets[3021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788113e-a737-4839-823d-5d385bb56b48",
   "metadata": {},
   "source": [
    "### Concatenate documents for vocab generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c9ff05-9df9-4f51-91b4-f93a62e6d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = biden_tweets + trump_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95b19c-1577-434f-ab0d-6c13f07337cb",
   "metadata": {},
   "source": [
    "## (20 pts) Task I: Train a Doc2Vec model (using the Gensim package) on tweets from the 2020 US presidential election\n",
    "\n",
    "*Docs*: \n",
    "\n",
    "* https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "*Useful tutorials*: \n",
    "\n",
    "* https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial \n",
    "* https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43e854-6a1c-432d-a125-97e10849e49d",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f67cb3d6-26c8-455f-b5e6-7f6685ba8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change as needed\n",
    "K = 20\n",
    "word_frequency_threshold = 2\n",
    "epochs = 10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27e641c1-7e76-4e15-bd86-b79b4460adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "documents = [TaggedDocument(all_tweets, [i]) for i, all_tweets in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents,vector_size=K, min_count=word_frequency_threshold, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a46bd154",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titlelist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-2fdf3e3d6681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tag'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitlelist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'titlelist' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = [doc2vec.TaggedDocument(sentence, 'tag') for sentence in titlelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f69d8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_split = [row.split() for row in all_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4ad3f55-7428-4278-82e9-c7243f75498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "#Build vocabulary\n",
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "469fdd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(documents,total_examples=model.corpus_count, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42cdf23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01795956  0.01144458 -0.0233179  -0.00772732 -0.00205748  0.02062779\n",
      "  0.00868887 -0.00773061 -0.00015735 -0.01275906  0.01478313  0.00916422\n",
      "  0.01442062 -0.0109679  -0.0140332   0.01916066 -0.00834236 -0.01888616\n",
      "  0.01555071  0.00806334]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb31c34-61b7-49b3-a31d-ad7c742a2876",
   "metadata": {},
   "source": [
    "## (10  pts) Task II: Evaluate your model by computing the most similar documents (tweets) to new (perhaps made up) tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7d2729a-a9ba-4daf-ab93-06936ebd16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template function\n",
    "def find_similar_tweets(tweet, top_n=10):\n",
    "    doc_vector = model.infer_vector(tweet)\n",
    "    sims = model.dv.most_similar([doc_vector], topn=top_n)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1aaf0a8a-0b0c-447c-a619-96780f8a750c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.30320504307746887),\n",
       " (0, 0.24533195793628693),\n",
       " (3, 0.2099137157201767),\n",
       " (7, 0.10132831335067749),\n",
       " (6, 0.08364967256784439),\n",
       " (5, 0.022875366732478142),\n",
       " (1, -0.015743907541036606),\n",
       " (4, -0.05227864533662796),\n",
       " (8, -0.3419002592563629)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "find_similar_tweets(['Trump', 'is', 'an', 'asshole'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99900417-805a-415b-bbd4-ee456d1f9e5a",
   "metadata": {},
   "source": [
    "## (10 pts extra credit) Task III: Produce a scatter plot of the compressed document embeddings (2D or 3D)\n",
    "\n",
    "*Useful resources*:\n",
    "\n",
    "* http://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd360b-ea53-479f-bfbe-bc4f82268daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
