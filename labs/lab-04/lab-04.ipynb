{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2640818d-560f-4bda-b8c5-dd75ebbe1ec8",
   "metadata": {},
   "source": [
    "# Lab 04: Extracting topics from research articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eaeccb-cf1f-47e1-8dfb-06ac139bd64d",
   "metadata": {},
   "source": [
    "# Part I: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fd692-eeb5-4dfa-b64b-05fd32b5a1c6",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "- Source: https://www.kaggle.com/blessondensil294/topic-modeling-for-research-articles/version/1\n",
    "- Download: https://georgetown.box.com/s/1qkrvdewe8ez35f2asblxysh136dvh6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec830e7-b20d-469b-a552-34d6f4f8cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('research-articles-dataset/train.csv').sample(frac=1)\n",
    "test_data = pd.read_csv('research-articles-dataset/test.csv').sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d122fd-be0d-411d-bd06-376cec211ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fda973-f99a-479d-bd36-412183d3e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in train_data.columns[3:]:\n",
    "    print(f\"{topic}: {sum(train_data[topic]) / len(train_data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c93ce6-0986-4b37-b7c6-14caf6002c6e",
   "metadata": {},
   "source": [
    "### Reuse Spacy pipeline from Lab-02 for text normalization & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a87b5-e8d5-4968-b065-bccb27479b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "M = 1000\n",
    "\n",
    "pipeline = spacy.load('en_core_web_sm')\n",
    "\n",
    "# http://emailregex.com/\n",
    "email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "\n",
    "# replace = [ (pattern-to-replace, replacement),  ...]\n",
    "replace = [\n",
    "    (r\"<a[^>]*>(.*?)</a>\", r\"\\1\"),  # Matches most URLs\n",
    "    (email_re, \"email\"),            # Matches emails\n",
    "    (r\"(?<=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n",
    "    (r\"\\d+\", \"number\"),              # Map digits to special token <numbr>\n",
    "    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"), # Punctuation and other junk\n",
    "    (r\"\\s+\", \" \")                   # Stips extra whitespace\n",
    "]\n",
    "\n",
    "train_sentences = []\n",
    "for i, d in enumerate(train_data['ABSTRACT'][:M]):\n",
    "    for repl in replace:\n",
    "        d = re.sub(repl[0], repl[1], d)\n",
    "    train_sentences.append(d)\n",
    "\n",
    "\n",
    "@Language.component(\"lab04Preprocessor\")\n",
    "def ng20_preprocess(doc):\n",
    "    tokens = [token for token in doc \n",
    "              if not any((token.is_stop, token.is_punct))]\n",
    "    tokens = [token.lemma_.lower().strip() for token in tokens]\n",
    "    tokens = [token for token in tokens if token]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "pipeline.add_pipe(\"lab04Preprocessor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c19c8-80ec-4bb2-a7bd-aaad024d4e7b",
   "metadata": {},
   "source": [
    "### Pass data through our Spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b68ad3-f1a4-4f2a-b3a5-7f9fc0456ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for sent in train_sentences[:M]:\n",
    "    docs.append(pipeline(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36b399-6417-4c64-a28f-b9ace7cc4a16",
   "metadata": {},
   "source": [
    "### Compute number of unique words (vocabulary size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21867e-25ef-4118-8e64-7abc6fb24b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set(\" \".join(docs).split(\" \")))\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9064a1-c13c-4510-b5be-fc8b6a37c6a6",
   "metadata": {},
   "source": [
    "# Part 2: Build Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97d061-e99f-4675-9c3a-7fa65350384e",
   "metadata": {},
   "source": [
    "### Build the term-document matrix (i.e., BOW features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95cfcc-ef76-4465-8ff2-d17bd12f9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# featurizer = CountVectorizer(max_features=vocab_size, max_df=0.95, min_df=0.005, stop_words='english')\n",
    "featurizer = TfidfVectorizer(max_features=vocab_size, max_df=0.95, stop_words='english')\n",
    "X = featurizer.fit_transform(docs)\n",
    "type(X), X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b30a34-698f-46bf-bc50-890a0dd50a1f",
   "metadata": {},
   "source": [
    "### Create a index-to-word map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f86bd6c-8a0a-4c9c-bd51-2896ec3fb5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx: word for word, idx in featurizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf9c1a-46eb-488e-a734-d35381c5d7ca",
   "metadata": {},
   "source": [
    "### Number of topics hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32a2ed-9ab2-4103-806b-90d4e3a57152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e7176-a6b0-48f1-99ca-2d49cf186862",
   "metadata": {},
   "source": [
    "### Plotting subroutine to visualize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1c81d-0b21-47f9-8a35-61d435e8dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model: sklearn topic model\n",
    "        feature_names: featurizer.get_feature_names()\n",
    "        n_top_words: N most relevant words in topic\n",
    "        title: plot title (such as model name)\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    cols = 5\n",
    "    rows = K // cols + K % cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 4 * rows), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[::-1][:n_top_words]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx + 1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be137c12-6647-40c9-8566-61f3ba0001a1",
   "metadata": {},
   "source": [
    "### (10 pts) Task I: Build a LSA model\n",
    "\n",
    "In this task we are going to build a LSA topic model from scratch. From lecture-04, we learned about LSA from the perspective of document retrieval. For document retrieval, you'll recall that we computed a truncated SVD by choosing some number of dimension $K << N$. This gave us the left singular column vectors, $\\mathbf{U} \\in \\mathbb{R}^{N \\times K}$, and the diagonal singular value matrix, $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{K \\times K}$ that we needed in order to project our queries, $\\mathbf{q} \\in \\mathbb{R}^{N}$, and documents, $\\mathbf{d} \\in \\mathbb{R}^{N}$, into $\\mathbb{R}^{K}$ space. Recall that the operation to do that was:\n",
    "\n",
    "$$\\hat{\\mathbf{q}} = \\mathbf{q}\\mathbf{U}\\mathbf{\\Sigma}^{-1} $$\n",
    "\n",
    "In this task, we're going to evaluate the singular values, $\\sigma_{i,j}$ in $\\mathbf{\\Sigma}$, and their corresponding basis vectors, $\\mathbf{u}^{(j)}$, in $\\mathbf{U}$, to extract the principal themes in the data. Execute the following subtasks.\n",
    "\n",
    "1. For each column vector, print out the top 10 most relevant words.\n",
    "2. Visualize the top 10 words using the `plot_topics()` function provided above.\n",
    "3. What affect does the hyperparameter $K$ have on the result?\n",
    "4. Is there a principled way to determine an appropriate value for $K$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ef0ca-faf6-4703-ae8e-40b619ec2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c130a66-7c6e-42c0-b6c2-b7dee5ba993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3b782-5ced-480d-9abc-b88bc722747a",
   "metadata": {},
   "source": [
    "### (5 pts) Task 2: Perform topic extraction using the NMF and LDA models from sklearn\n",
    "\n",
    "In this task we perform topic extraction using the Non-negative Matrix Factorization (NMF) and Latent Dirichlet Allocation (LDA) models provided by sklearn. \n",
    "\n",
    "1. Fit the NMF and LDA models in the provided cells below.\n",
    "2. Visualize the results using the `plot_topics` function.\n",
    "3. How do the results compare to your home-spun LSA topic model?\n",
    "4. What are the differences between these model that might give rise to these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62ac18-e738-47c8-86fc-2279bff226a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc47dd-0893-414b-8dc5-0a9d1ac0493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673115-7d0e-467b-93ee-7a32dc5c4e15",
   "metadata": {},
   "source": [
    "### (5 pts) Task 3: Map the test data onto the topic spaces learned from the LSA, NMF, and LDA models\n",
    "\n",
    "The purpose of this task is to assign topics to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c6a3d-3e8d-48bf-a598-7393aad8789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
