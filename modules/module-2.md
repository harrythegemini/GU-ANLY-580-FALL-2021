# Module 2: Distributional Semantics

In this module we will use unsupervised feature learning approaches to model similarity between pieces of text, whether it be single words or sequences of words, and apply them to information retrieval and topic modeling. 

### Module Topics

1. The distributional hypothesis

2. Semantic relevance, informativeness, Zipf's Law, and the TF-IDF scoring method

3. Low-rank matrix factorization

4. Topic modeling

5. Similarity measures

6. CBOW & skip-gram modeling

7. Word2Vec


### Outcomes

You will leave this module with the following skills / knowledge:

1. Understand Zipf's law and how it informs the scoring mechanism in TF-IDF weighting.

2. Be able to apply TF-IDF to document retrieval

3. Understand matrix factorization using SVD, and be able to implement it numerically

4. Understand the modeling approaches/assumptions behind LSA, pLSA, LDA, and NMF topic modeling techniques, and be able to apply them to topic extraction from raw text.

5. Understand and be able to recite the distributional hypothesis

6. Understand and be able to implement several similarity metrics such as inner product, cosine similarity, L2 distance, L1 distance, Levenshtein distance.

7. Understand the concept behind the Continuous Bag of Words (CBOW) and skip gram modeling approaches.

8. Understand and be able to implement Word2Vec using the modeling approaches from (7). Understand the core problems that arise when training these models, and how Word2Vec addresses them.


#
### Due Dates

- Assignment 2
    - Section 1: Oct 17 11:59pm EST
    - Section 2: Oct 06 11:59pm EST

- Labs 4,5
    - Section 1: Dec 08 11:59pm EST
    - Section 2: Dec 08 11:59pm EST
