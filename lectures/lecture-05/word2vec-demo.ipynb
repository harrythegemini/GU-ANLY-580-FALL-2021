{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1842f9-e117-45c7-8024-a27a28a6a162",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training a Word2Vec model on the Wikipedia page about the *United States*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8c073-1114-49dc-9d86-11ca27419626",
   "metadata": {},
   "source": [
    "### Grab the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e0d622-c73f-407a-836d-b95b3333b0b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://en.wikipedia.org/wiki/United_States\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b2ed0b-75c7-4c0c-bcde-db04821d75e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing pipeline (repurposed from Lab-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda56c86-770c-44f1-ab49-0a61a7c082f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "pipeline_name = 'WikiUS'\n",
    "\n",
    "@Language.component(pipeline_name)\n",
    "def preprocess(doc):\n",
    "    doc = [token for token in doc if not token.is_punct]\n",
    "    doc = [token for token in doc if not token.is_stop]\n",
    "    doc = [token.text.lower().strip() for token in doc]\n",
    "    doc = [token for token in doc if 0 < len(token) <= 12]\n",
    "    return \" \".join(doc)\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \n",
    "    # http://emailregex.com/\n",
    "    email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)\n",
    "    *|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]\n",
    "    |\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9]\n",
    "    (?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}\n",
    "    (?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:\n",
    "    (?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "    # replace = [ (pattern-to-replace, replacement),  ...]\n",
    "    replace = [\n",
    "        (\"<[^>]*>\", \" \"),\n",
    "        (email_re, \" \"),                           # Matches emails\n",
    "        (r\"(?<=\\d),(?=\\d)\", \"\"),                   # Remove commas in numbers\n",
    "        (r\"\\d+\", \" \"),                             # Map digits to special token <numbr>\n",
    "        (r\"[*\\^\\.$&@<>,\\-/+{|}=?#:;'\\\"\\[\\]]\", \"\"), # Punctuation and other junk\n",
    "        (r\"[\\n\\t\\r]\", \" \"),                        # Removes newlines, tabs, creturn\n",
    "        (r\"[^\\x00-\\x7F]+\", \"\"),                    # Removes non-ascii chars\n",
    "        (r\"\\\\+\", \" \"),                             # Removes double-backslashs\n",
    "        (r\"\\s+n\\s+\", \" \"),                         # 'n' leftover from \\\\n\n",
    "        (r\"\\s+\", \" \")                              # Strips extra whitespace\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline = spacy.load('en_core_web_sm')\n",
    "        self.pipeline.add_pipe(pipeline_name);\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.transform(*args, **kwargs)\n",
    "\n",
    "    def transform(self, doc: str):\n",
    "        for repl in self.replace:\n",
    "            doc = re.sub(repl[0], repl[1], doc)\n",
    "        return self.pipeline(doc)\n",
    "\n",
    "\n",
    "pipeline = Pipeline();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b3e80-d257-477e-b87e-cecf0261d4ba",
   "metadata": {},
   "source": [
    "### Normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5dd659-bff9-4aa1-b4ae-e41e2d7a0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "i = 0\n",
    "while i < len(r.text):\n",
    "    text_chunk = r.text[i:i + int(1e5)]\n",
    "    docs.append(pipeline(text_chunk))\n",
    "    i += int(1e5)\n",
    "doc = \" \".join(docs);\n",
    "\n",
    "# Wiki articles have many citations\n",
    "doc = doc.replace('citeref', '').replace(\"tmulti\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cf074-3aa9-4c4c-9dae-ade5f4e99827",
   "metadata": {},
   "source": [
    "### Excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881aad54-8cf4-4179-be36-8b7d639a0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[65000:70000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178caea-6edc-4861-bd23-ddc1d82f2eff",
   "metadata": {},
   "source": [
    "### Construct a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe578e7-da7b-48a3-9ca8-411ee03d2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "doc = doc.split(\" \")\n",
    "words_histogram = Counter(doc)\n",
    "\n",
    "freq_threshold = 2\n",
    "vocab = {}\n",
    "for word, count in sorted(words_histogram.items(), \n",
    "                          key=lambda wrd_cnt: wrd_cnt[0]):\n",
    "    if word and count >= freq_threshold:\n",
    "        vocab[word] = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887ca20-e696-4229-a3b2-6df260a77940",
   "metadata": {},
   "source": [
    "### Remove OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49040993-ad1f-4a9c-a185-85c9ca1b0445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = [word for word in doc if vocab.get(word) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7563d-ad27-4176-9a4b-ec5585e04b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size\n",
    "N = len(vocab)\n",
    "\n",
    "# Training set size\n",
    "M = len(doc)\n",
    "\n",
    "M, N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921216f1-aa8d-4ce9-a6e9-c8f54d30a121",
   "metadata": {},
   "source": [
    "# Build a word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1696d-4288-4144-b77b-aaebe148616f",
   "metadata": {},
   "source": [
    "### Softmax (numerically stable version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7eb309-567b-4d00-884f-fecbb33a8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(Z) -> np.ndarray:\n",
    "    Z_exp = np.exp(Z - np.max(Z))\n",
    "    partition = np.sum(Z_exp)\n",
    "    return Z_exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f07786-3439-4cb5-9757-0700d7a6abb1",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5203d-11b2-4bd8-8110-c9c0c75a85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding dimension (K << N)\n",
    "K = 15\n",
    "\n",
    "# Number of passes through the data\n",
    "epochs = 50\n",
    "\n",
    "# The number of words on both sides\n",
    "# of center word to consider as \n",
    "# context\n",
    "context_window = 6\n",
    "\n",
    "# Learning rate for gradient updates\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a8f89-4f17-4c6f-860d-33d2974d10ab",
   "metadata": {},
   "source": [
    "### Construct a projection matrix shape = (K x N)\n",
    "\n",
    "Note: W is our projection matrix, w represents a word (these are different things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cd22e5-8bc1-4528-bf97-ede957637bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding():\n",
    "    \"\"\"\n",
    "    Make a randomly initialized embedding \n",
    "    matrix of shape K x N\n",
    "    \"\"\"\n",
    "    return np.random.random((K, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d91d1-4699-47e2-be91-68df85b3727d",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da20395-1321-4d22-b408-d8fce06eeed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Word embeddings\n",
    "U = initialize_embedding()\n",
    "\n",
    "# Context embeddings\n",
    "V = initialize_embedding()\n",
    "\n",
    "# One-hot encoded word representations\n",
    "X = np.eye(N)\n",
    "\n",
    "with tqdm(total=int(epochs * (M - 2 * context_window))) as bar:\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "        ce_losses = []\n",
    "        acc = []\n",
    "        \n",
    "        for i in range(context_window, M - context_window, 1):\n",
    "            \n",
    "            word_doc_idx = i\n",
    "            word = doc[word_doc_idx]\n",
    "            word_idx = vocab.get(word, 0)\n",
    "            \n",
    "            context_words = doc[i - context_window: i + context_window + 1]\n",
    "            context_words = np.delete(context_words, context_window)\n",
    "            context_idxs = [vocab.get(context_word, 0) for context_word in context_words]        \n",
    "            \n",
    "            # print(word, context_words)\n",
    "            \n",
    "            # One-hot encoded center word (1 x N)\n",
    "            x_w = X[word_idx]\n",
    "            \n",
    "            # One-hot encoded context words (2 * context window x N)\n",
    "            X_c = X[context_idxs]\n",
    "            \n",
    "            # Embedding for word w (1 x K)\n",
    "            u_w = U[:, word_idx]\n",
    "            \n",
    "            # Inner product between embedding for word w and embedding for all context words in V (1 x N)\n",
    "            Z = u_w.dot(V)\n",
    "            \n",
    "            # Probability distribution over all context words for center word w (1 x N)\n",
    "            P = softmax(Z)\n",
    "            \n",
    "            # Cross entropy loss\n",
    "            ce = -np.sum(np.log(P[context_idxs]))\n",
    "            ce_losses.append(ce)\n",
    "            \n",
    "            # Prediction errors (2 * context_window x N)\n",
    "            errors = P - X_c\n",
    "            \n",
    "            for error in errors:\n",
    "                \n",
    "                # Gradients w.r.t. U_w: dNLL/dU_w = sum(V.dot(P - X_c))\n",
    "                grad_U_w = V.dot(error.T) # (K x 1)\n",
    "\n",
    "                # Gradient w.r.t. V: dNLL/dV = u_w x P - X_c\n",
    "                grad_V = np.expand_dims(u_w, axis=1).dot(np.expand_dims(error, axis=0))\n",
    "                \n",
    "                # Gradient updates\n",
    "                U[:, word_idx] -= lr * grad_U_w.T\n",
    "                V -= lr * grad_V\n",
    "            \n",
    "            # Compute accuracy of context word being in top-50 from softmax probability\n",
    "            top = np.argsort(P)[-50:]\n",
    "            acc.append(sum([1 for idx in context_idxs if idx in top]) / (2 * context_window))\n",
    "            \n",
    "            bar.update()\n",
    "            \n",
    "        bar.set_description(\"epoch: %d, ce-loss: %.4f, acc: %.4f\" %\n",
    "                    (epoch + 1, np.mean(ce_losses), np.mean(acc)))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6135a5ac-14bc-443e-9c94-6f4065fb4761",
   "metadata": {},
   "source": [
    "### Compute most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c8330-3a3a-4a9f-ab65-9a761d440716",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlookup = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "Unormed = U / np.expand_dims(np.linalg.norm(U, axis=0, ord=2), axis=0)\n",
    "\n",
    "def compute_top_n_similar_words(word, n=20, method='dot'):\n",
    "    idx = vocab[word]\n",
    "    if method == 'dot':\n",
    "        neighbors_idxs = np.argsort(U[:, idx].dot(U))[-n:][::-1]\n",
    "    elif method == 'cosine':\n",
    "        neighbors_idxs = np.argsort(Unormed[:, idx].dot(Unormed))[-n:][::-1]\n",
    "    neighbors = [wordlookup[idx] for idx in neighbors_idxs]\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a8039-d438-4387-9c85-3b2ea3e79579",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['budget', 'cia', 'minnesota', 'suffrage', 'civil', \n",
    "         'war', 'election', 'declaration', 'president', 'obama', \n",
    "         'trump', 'democrat', 'republican', 'baseball', 'west',\n",
    "         'vice', 'speaker', 'pelosi', 'iraq', 'games', 'supreme',\n",
    "         'abortion', 'left', 'right', 'senate', 'house']\n",
    "\n",
    "for word in words:\n",
    "    print(word, \": \", compute_top_n_similar_words(word, n=10, method='cosine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48b7f6-df40-4c24-8fdb-3af8e5226bfc",
   "metadata": {},
   "source": [
    "### Compute analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0462d-a252-4715-9eeb-6a5380b32b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_analogy(word1, word2, word3):\n",
    "    \"\"\"\n",
    "    Desired behavior:\n",
    "        word2 - word1 + word3 = word4\n",
    "        king - man + woman = queen\n",
    "    \"\"\"\n",
    "    u1 = Unormed[:, vocab[word1]]\n",
    "    u2 = Unormed[:, vocab[word2]]\n",
    "    u3 = Unormed[:, vocab[word3]]\n",
    "    u4 = u2 - u1 + u3\n",
    "    u4_idxs = np.argsort(u4.dot(Unormed))[-10:][::-1]\n",
    "    word4_candidates = [wordlookup[idx] for idx in u4_idxs]\n",
    "    return word4_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b87da5-6041-4349-b5c2-5165c3915b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogies = [('mitch', 'senate', 'nancy'),\n",
    "             ('obama', 'president', 'kamala'),\n",
    "             ('adams', 'england', 'jefferson')]\n",
    "\n",
    "for analogy in analogies:\n",
    "    print(compute_analogy(*analogy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ce81e-dd4c-4d04-aaba-9eb5155a37e4",
   "metadata": {},
   "source": [
    "# Visualize embeddings using Tensorboard Projector\n",
    "\n",
    "https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab694e59-6b5a-448d-8adc-09124e8a0522",
   "metadata": {},
   "source": [
    "### Save embeddings/vocab to disk\n",
    "\n",
    "Use the cell below to save your embeddings and vocab to disk (`vectors.tsv`, `metadata.tsv`). The embedding projector expects data in the following format:\n",
    "\n",
    "\n",
    "\n",
    "`vectors.tsv` (N=3 K=4 embeddings):\n",
    "\n",
    "    0.1\\t0.2\\t0.5\\t0.9\n",
    "    0.2\\t0.1\\t5.0\\t0.2\n",
    "    0.4\\t0.1\\t7.0\\t0.8\n",
    "\n",
    "\n",
    "`metadata.tsv` (N=3 word vocabulary):\n",
    "\n",
    "    three\n",
    "    word\n",
    "    vocabulary\n",
    "\n",
    "You can use the helper functions in the cell below to save and load embeddings/vocab to/fro disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c800ff-8d3b-42e1-87e5-b0dc8272c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_matrix(matrix, fpath):\n",
    "    D1, D2 = matrix.shape\n",
    "    tsv = \"\"\n",
    "    for i in range(D1):\n",
    "        for j in range(D2):\n",
    "            tsv += str(matrix[i, j]) + '\\t'\n",
    "        tsv = tsv.strip('\\t') + '\\n'\n",
    "    tsv = tsv.strip('\\n')\n",
    "    with open(fpath, \"w\") as fd:\n",
    "        fd.write(tsv)\n",
    "\n",
    "        \n",
    "def load_matrix(fpath):\n",
    "    matrix = []\n",
    "    with open(fpath, 'r') as fd:\n",
    "        tsv = fd.read()\n",
    "    for line in tsv.split('\\n'):\n",
    "        row = []\n",
    "        for value in line.split('\\t'):\n",
    "            row.append(float(value))\n",
    "        matrix.append(row)\n",
    "    return np.array(matrix)\n",
    "\n",
    "        \n",
    "def save_vocab(vocab: dict, fpath):\n",
    "    tsv = \"\"\n",
    "    for word, idx in sorted(vocab.items(), key=lambda item: item[1]):\n",
    "        tsv += word + '\\n'\n",
    "    tsv = tsv.strip('\\n')\n",
    "    with open(fpath, \"w\") as fd:\n",
    "        fd.write(tsv)\n",
    "\n",
    "\n",
    "def load_vocab(fpath):\n",
    "    with open(fpath, \"r\") as fd:\n",
    "        tsv = fd.read()\n",
    "    vocab = {}\n",
    "    for line in tsv.split('\\n'):\n",
    "        vocab[line.strip()] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4840a-26a2-4f85-a19f-b5c81edff4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_matrix(U.T, \"vectors.tsv\")\n",
    "save_vocab(vocab, \"metadata.tsv\")\n",
    "\n",
    "len(load_vocab(\"metadata.tsv\")), load_matrix(\"vectors.tsv\").T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d387b12-cc58-4ebe-892b-723b75a171c5",
   "metadata": {},
   "source": [
    "### Instructions for uploading data into the Projector\n",
    "\n",
    "Click the load button called *load* on the left side of the screen. You will then be given the option to upload two files (image below). Upload your `vectors.tsv` and `metadata.tsv` files from the last step and then click out of the pop up (oddly the pop-up does not close once your files have been uploaded).\n",
    "\n",
    "<img src=\"projector-load.png\" alt=\"Embedding/Vocab upload\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a1a61-606c-44f3-b8ff-a26d9b769f1b",
   "metadata": {},
   "source": [
    "### Instructions for using the Projector\n",
    "\n",
    "There are three compression algorithms that you can use, I recommend tSNE with the default settings. After ~2000 iterations tSNE should yield words clustered in a way that loosly reflects underlying semantic relationships. There is a search bar on the right; baked into the UI is a entity tagger that automatically tags words from `metadata.tsv` with entities (if one is recognized). When you type in the name of a state for example, the Projector will highlight the other words tagged with <STATE>, and you will a tight clustering of states in embeddings space (below).\n",
    "\n",
    "<img src=\"projector.png\" alt=\"Embedding projector snapshot\" width=\"1000\" height=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
